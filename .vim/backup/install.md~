# Install Hadoop

## Prepare
### Get package

'''
wget -c http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
'''

### publickey authentication

'''
ssh-keygen

ssh-copy-id localhost
'''

### Modify /etc/hosts

Add these lines.

'''
127.0.0.1 spark         # somebody say this must be annotated.
192.168.2.45 spark
'''

## Install
### Install JAVA and configure environment variables.

'''
export JAVA_HOME=/opt/jdk1.7.0_75
export PATH=$JAVA_HOME/bin:$PATH
'''

### Modify hadoop config

'''
cd hadoop-2.7.2
vim etc/hadoop/hadoop-env.sh
'''

Add these lines:

'''
export JAVA_HOME=/opt/jdk1.7.0_75
export HADOOP_COMMON_HOME=/root/hadoop-2.7.2
'''

### Add bin to Path
'''
PATH=$PATH:$HOME/bin:/root/hadoop-2.7.2/sbin:/root/hadoop-2.7.2/bin
'''

## Local mode validation

'''
mkdir input
cp etc/hadoop/*.xml input
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar grep input output 'dfs[a-z.]+'
cat output/*
'''

'''
1   dfsadmin
'''

## Pseudo-distributed

### Modify configure


'''
vim etc/hadoop/core-site.xml
'''
Add these lines:

'''
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
</property>

'''

'''
vim etc/hadoop/hdfs-site.xml
'''

Add these lines:

'''
<property>
    <name>dfs.replication</name>
    <value>1</value>
</property>
'''

### Format namenode

'''
hdfs namenode -format
'''

### Startup

'''
start-dfs.sh
'''

### Validation
http://localhost:50070/

## Configure Yarn

### Modify config

'''
cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml
vim etc/hadoop/mapred-site.xml
'''

Add these lines:

'''
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
</property>
'''

'''
vim etc/hadoop/yarn-site.xml
'''

Add these lines:
'''
<property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
</property>
'''

### Startup yarn

'''
start-yarn.sh
'''

### Validation

'''
http://localhost:8088/
'''
